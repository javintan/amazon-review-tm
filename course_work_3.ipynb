{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GROUP 12 Coursework 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:33:33.585936Z",
     "start_time": "2020-02-27T05:33:31.961677Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:33:34.337359Z",
     "start_time": "2020-02-27T05:33:33.588401Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Digital_Music_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Binary label\n",
    "From the rating column, compute a binary label. I will use a cutoff threshold $C$. Ratings greater or equal to $C$ will be assigned label 1, ratings less than $C$ will be assigned label 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:33:34.968460Z",
     "start_time": "2020-02-27T05:33:34.964899Z"
    }
   },
   "outputs": [],
   "source": [
    "C = 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:33:35.151091Z",
     "start_time": "2020-02-27T05:33:35.142648Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Label'] = (df.Rating >= C).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:33:35.684591Z",
     "start_time": "2020-02-27T05:33:35.616824Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Time</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>It's hard to believe \"Memory of Trees\" came ou...</td>\n",
       "      <td>Enya's last great album</td>\n",
       "      <td>09 12, 2006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A clasically-styled and introverted album, Mem...</td>\n",
       "      <td>Enya at her most elegant</td>\n",
       "      <td>06 3, 2001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I never thought Enya would reach the sublime h...</td>\n",
       "      <td>The best so far</td>\n",
       "      <td>07 14, 2003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This is the third review of an irish album I w...</td>\n",
       "      <td>Ireland produces good music.</td>\n",
       "      <td>05 3, 2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Enya, despite being a successful recording art...</td>\n",
       "      <td>4.5; music to dream to</td>\n",
       "      <td>01 17, 2008</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64701</th>\n",
       "      <td>64701</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I like the reggae sound a lot in this song. I ...</td>\n",
       "      <td>Cool song</td>\n",
       "      <td>06 24, 2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64702</th>\n",
       "      <td>64702</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I first heard this on Sirius and had to have i...</td>\n",
       "      <td>Great Song</td>\n",
       "      <td>07 9, 2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64703</th>\n",
       "      <td>64703</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I absolutely love this song, it downloaded fin...</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>07 13, 2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64704</th>\n",
       "      <td>64704</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Reggae, island beats aren't really my cup of t...</td>\n",
       "      <td>Well-crafted song</td>\n",
       "      <td>07 9, 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64705</th>\n",
       "      <td>64705</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Magic! is a Canadian band that incorporates re...</td>\n",
       "      <td>Souless Reggae</td>\n",
       "      <td>07 18, 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64705 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Rating                                             Review  \\\n",
       "0               0     5.0  It's hard to believe \"Memory of Trees\" came ou...   \n",
       "1               1     5.0  A clasically-styled and introverted album, Mem...   \n",
       "2               2     5.0  I never thought Enya would reach the sublime h...   \n",
       "3               3     5.0  This is the third review of an irish album I w...   \n",
       "4               4     4.0  Enya, despite being a successful recording art...   \n",
       "...           ...     ...                                                ...   \n",
       "64701       64701     4.0  I like the reggae sound a lot in this song. I ...   \n",
       "64702       64702     5.0  I first heard this on Sirius and had to have i...   \n",
       "64703       64703     5.0  I absolutely love this song, it downloaded fin...   \n",
       "64704       64704     3.0  Reggae, island beats aren't really my cup of t...   \n",
       "64705       64705     1.0  Magic! is a Canadian band that incorporates re...   \n",
       "\n",
       "                            Summary         Time  Label  \n",
       "0           Enya's last great album  09 12, 2006      1  \n",
       "1          Enya at her most elegant   06 3, 2001      1  \n",
       "2                   The best so far  07 14, 2003      1  \n",
       "3      Ireland produces good music.   05 3, 2000      1  \n",
       "4            4.5; music to dream to  01 17, 2008      1  \n",
       "...                             ...          ...    ...  \n",
       "64701                     Cool song  06 24, 2014      1  \n",
       "64702                    Great Song   07 9, 2014      1  \n",
       "64703                    Five Stars  07 13, 2014      1  \n",
       "64704             Well-crafted song   07 9, 2014      0  \n",
       "64705                Souless Reggae  07 18, 2014      0  \n",
       "\n",
       "[64705 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(subset=['Review'])  # Drop empty Review\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:33:36.453891Z",
     "start_time": "2020-02-27T05:33:36.444041Z"
    }
   },
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "def preprocess(text):\n",
    "    #text is a document of a corpus\n",
    "    #lowercase of text\n",
    "    #remove all the irrelevant numbers and punctuation\n",
    "    words = word_tokenize(re.sub(r'[^a-z]+', ' ', text.lower()))\n",
    "    #remove the meaningless stopping words\n",
    "    words = [t for t in words if t not in sw]\n",
    "    #stemming transformation\n",
    "    words = [stemmer.stem(t) for t in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:37:42.599730Z",
     "start_time": "2020-02-27T05:33:36.908018Z"
    }
   },
   "outputs": [],
   "source": [
    "# Takes time\n",
    "df['Tokens'] = df.Review.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:37:42.612282Z",
     "start_time": "2020-02-27T05:37:42.603279Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('clean_data.pkl', 'wb') as f:\n",
    "#     pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct loading of cleaned data\n",
    "To avoid long awaiting time start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T04:30:40.353818Z",
     "start_time": "2020-02-27T04:30:38.361757Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('clean_data.pkl', 'rb') as f:\n",
    "#     df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Estimate a LASSO model\n",
    "Predict the rating using text features. Tune regularization hyper-parameter by CV. How many terms got selected? Which five have the largest coefficients? which five have the smallest coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:37:48.846599Z",
     "start_time": "2020-02-27T05:37:42.618794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of features 1335\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=0.01)\n",
    "dfm = vectorizer.fit_transform(list(df.Tokens.apply(' '.join)))\n",
    "X_review_text_feature = dfm.toarray()\n",
    "print(\"# of features\", X_review_text_feature.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:37:48.854680Z",
     "start_time": "2020-02-27T05:37:48.849903Z"
    }
   },
   "outputs": [],
   "source": [
    "y = df.Label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:37:49.229216Z",
     "start_time": "2020-02-27T05:37:48.857787Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split Train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_review_text_feature, y, test_size=0.1,\n",
    "                                                   random_state=0)  # Ensure consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:38:02.227845Z",
     "start_time": "2020-02-27T05:37:49.231315Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save train test data for R\n",
    "pd.DataFrame(data=X_train, columns=vectorizer.get_feature_names()).to_csv('X_train.csv', index=False)\n",
    "pd.DataFrame(data=X_test, columns=vectorizer.get_feature_names()).to_csv('X_test.csv', index=False)\n",
    "pd.DataFrame(data=y_train, columns=['label']).to_csv('y_train.csv', index=False)\n",
    "pd.DataFrame(data=y_test, columns=['label']).to_csv('y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:44:51.600875Z",
     "start_time": "2020-02-27T05:44:51.595555Z"
    }
   },
   "outputs": [],
   "source": [
    "# CV a LASSO\n",
    "lasso_logistic = LogisticRegressionCV(Cs=np.arange(0.001, 2, 0.1),\n",
    "                                      cv=5,\n",
    "                                      penalty='l1',  # LASSO\n",
    "                                      n_jobs=-1,  # Use all processors\n",
    "                                      verbose=1,  # Report convergence\n",
    "                                      solver='saga',  # Suppose L1 penalty\n",
    "                                      tol=0.1, # Tolerance cannot be too low, take too much time,\n",
    "                                      random_state=0, # Ensure reproductibility\n",
    "                                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:47:11.940509Z",
     "start_time": "2020-02-27T05:44:54.294794Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 5 epochs took 6 seconds\n",
      "convergence after 5 epochs took 6 seconds\n",
      "convergence after 6 epochs took 7 seconds\n",
      "convergence after 6 epochs took 7 seconds\n",
      "convergence after 7 epochs took 12 seconds\n",
      "convergence after 7 epochs took 13 seconds\n",
      "convergence after 7 epochs took 13 seconds\n",
      "convergence after 7 epochs took 13 seconds\n",
      "convergence after 2 epochs took 4 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 4 seconds\n",
      "convergence after 2 epochs took 4 seconds\n",
      "convergence after 2 epochs took 4 seconds\n",
      "convergence after 2 epochs took 4 seconds\n",
      "convergence after 2 epochs took 4 seconds\n",
      "convergence after 2 epochs took 4 seconds\n",
      "convergence after 2 epochs took 4 seconds\n",
      "convergence after 2 epochs took 4 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 4 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 4 seconds\n",
      "convergence after 2 epochs took 4 seconds\n",
      "convergence after 2 epochs took 4 seconds\n",
      "convergence after 2 epochs took 5 seconds\n",
      "convergence after 2 epochs took 4 seconds\n",
      "convergence after 2 epochs took 4 seconds\n",
      "convergence after 2 epochs took 4 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 4 seconds\n",
      "convergence after 2 epochs took 4 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 3 seconds\n",
      "convergence after 2 epochs took 4 seconds\n",
      "convergence after 2 epochs took 4 seconds\n",
      "convergence after 2 epochs took 5 seconds\n",
      "convergence after 2 epochs took 5 seconds\n",
      "convergence after 2 epochs took 6 seconds\n",
      "convergence after 2 epochs took 5 seconds\n",
      "convergence after 2 epochs took 6 seconds\n",
      "convergence after 2 epochs took 6 seconds\n",
      "convergence after 2 epochs took 6 seconds\n",
      "convergence after 2 epochs took 7 seconds\n",
      "convergence after 2 epochs took 5 seconds\n",
      "convergence after 2 epochs took 6 seconds\n",
      "convergence after 6 epochs took 5 seconds\n",
      "convergence after 8 epochs took 8 seconds\n",
      "convergence after 2 epochs took 2 seconds\n",
      "convergence after 2 epochs took 1 seconds\n",
      "convergence after 2 epochs took 2 seconds\n",
      "convergence after 2 epochs took 2 seconds\n",
      "convergence after 2 epochs took 2 seconds\n",
      "convergence after 2 epochs took 2 seconds\n",
      "convergence after 2 epochs took 2 seconds\n",
      "convergence after 2 epochs took 2 seconds\n",
      "convergence after 2 epochs took 2 seconds\n",
      "convergence after 2 epochs took 2 seconds\n",
      "convergence after 2 epochs took 2 seconds\n",
      "convergence after 2 epochs took 2 seconds\n",
      "convergence after 2 epochs took 2 seconds\n",
      "convergence after 2 epochs took 2 seconds\n",
      "convergence after 2 epochs took 2 seconds\n",
      "convergence after 2 epochs took 2 seconds\n",
      "convergence after 2 epochs took 2 seconds\n",
      "convergence after 2 epochs took 1 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=array([1.000e-03, 1.010e-01, 2.010e-01, 3.010e-01, 4.010e-01, 5.010e-01,\n",
       "       6.010e-01, 7.010e-01, 8.010e-01, 9.010e-01, 1.001e+00, 1.101e+00,\n",
       "       1.201e+00, 1.301e+00, 1.401e+00, 1.501e+00, 1.601e+00, 1.701e+00,\n",
       "       1.801e+00, 1.901e+00]),\n",
       "                     class_weight=None, cv=5, dual=False, fit_intercept=True,\n",
       "                     intercept_scaling=1.0, l1_ratios=None, max_iter=100,\n",
       "                     multi_class='auto', n_jobs=-1, penalty='l1',\n",
       "                     random_state=0, refit=True, scoring=None, solver='saga',\n",
       "                     tol=0.1, verbose=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_logistic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:47:12.029330Z",
     "start_time": "2020-02-27T05:47:11.957765Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.901])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The CV-ed optimal model paramters\n",
    "lasso_logistic.C_  # The best C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 10% sample held out as test data, the best model selected by CV predicts 86% out of them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:47:12.137740Z",
     "start_time": "2020-02-27T05:47:12.091577Z"
    }
   },
   "outputs": [],
   "source": [
    "# Report non-zero coefs\n",
    "# Zip features and coefs\n",
    "feature_coef = pd.DataFrame(data={'feature': vectorizer.get_feature_names(),\n",
    "                                 'coef': lasso_logistic.coef_[0]})\n",
    "feature_coef['abs_coef'] = np.abs(feature_coef.coef)\n",
    "feature_coef = feature_coef.sort_values(by='abs_coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:47:12.167755Z",
     "start_time": "2020-02-27T05:47:12.148798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features 1335\n",
      "Prominent features 1192\n"
     ]
    }
   ],
   "source": [
    "print('Total features', feature_coef.shape[0])\n",
    "# Non zero coefs\n",
    "print('Prominent features' , feature_coef[feature_coef.abs_coef >= 0.01].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T16:13:39.639215Z",
     "start_time": "2020-02-26T16:13:39.631004Z"
    }
   },
   "source": [
    "Out of 1335 terms in the feature space, 1183 terms have a coefficient greater than 0.01.  \n",
    "The sparsity is not strong enforced due to CV-optimized paramter `C=1.901` (a bit large, i.e. regularization is a bit weak). But as we will see later, the logistic regression did a great job in identifying terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:47:12.261561Z",
     "start_time": "2020-02-27T05:47:12.176013Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coef</th>\n",
       "      <th>abs_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1317</th>\n",
       "      <td>worst</td>\n",
       "      <td>-0.952711</td>\n",
       "      <td>0.952711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>bore</td>\n",
       "      <td>-0.871155</td>\n",
       "      <td>0.871155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>decent</td>\n",
       "      <td>-0.725420</td>\n",
       "      <td>0.725420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>garbag</td>\n",
       "      <td>-0.715671</td>\n",
       "      <td>0.715671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>lack</td>\n",
       "      <td>-0.680506</td>\n",
       "      <td>0.680506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319</th>\n",
       "      <td>worthi</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>0.000119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>somehow</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>non</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>statu</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>nice</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1335 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature      coef  abs_coef\n",
       "1317    worst -0.952711  0.952711\n",
       "124      bore -0.871155  0.871155\n",
       "270    decent -0.725420  0.725420\n",
       "468    garbag -0.715671  0.715671\n",
       "647      lack -0.680506  0.680506\n",
       "...       ...       ...       ...\n",
       "1319   worthi -0.000119  0.000119\n",
       "1073  somehow  0.000109  0.000109\n",
       "797       non  0.000078  0.000078\n",
       "1112    statu  0.000075  0.000075\n",
       "793      nice -0.000010  0.000010\n",
       "\n",
       "[1335 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:47:12.302577Z",
     "start_time": "2020-02-27T05:47:12.273132Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coef</th>\n",
       "      <th>abs_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1317</th>\n",
       "      <td>worst</td>\n",
       "      <td>-0.952711</td>\n",
       "      <td>0.952711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>bore</td>\n",
       "      <td>-0.871155</td>\n",
       "      <td>0.871155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>decent</td>\n",
       "      <td>-0.725420</td>\n",
       "      <td>0.725420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>garbag</td>\n",
       "      <td>-0.715671</td>\n",
       "      <td>0.715671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>lack</td>\n",
       "      <td>-0.680506</td>\n",
       "      <td>0.680506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>horribl</td>\n",
       "      <td>-0.598200</td>\n",
       "      <td>0.598200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>terribl</td>\n",
       "      <td>-0.585822</td>\n",
       "      <td>0.585822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>unfortun</td>\n",
       "      <td>-0.573425</td>\n",
       "      <td>0.573425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>mediocr</td>\n",
       "      <td>-0.570304</td>\n",
       "      <td>0.570304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>favorit</td>\n",
       "      <td>0.561093</td>\n",
       "      <td>0.561093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature      coef  abs_coef\n",
       "1317     worst -0.952711  0.952711\n",
       "124       bore -0.871155  0.871155\n",
       "270     decent -0.725420  0.725420\n",
       "468     garbag -0.715671  0.715671\n",
       "647       lack -0.680506  0.680506\n",
       "558    horribl -0.598200  0.598200\n",
       "1181   terribl -0.585822  0.585822\n",
       "1244  unfortun -0.573425  0.573425\n",
       "730    mediocr -0.570304  0.570304\n",
       "410    favorit  0.561093  0.561093"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_coef.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The 5 most relevant terms to predict the rating are\n",
    "    - worst\n",
    "    - bore\n",
    "    - decent\n",
    "    - garbag\n",
    "    - lack\n",
    "- The 5 least relevant terms to predict the rating are\n",
    "    - worthi\n",
    "    - somehow\n",
    "    - non\n",
    "    - statu\n",
    "    - nice\n",
    "    \n",
    "As we have expect, the negative ('pejorative') terms in top 5 are unanimously associated with a negative coefficient. That is, the more frequent the word appears in a review, the more likely it will receive label 0 (rating <= 3). If we expand to the top 10 terms, we see that complimentary terms like \"favorit\" and \"awesom\" receive positive coefficients, on the opposite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 Estimate a Naive Bayes model\n",
    "- Devise a metric that identifies the term most associated with each class label.\n",
    "- How do these terms compare to those you identified in the previous question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:40:11.575199Z",
     "start_time": "2020-02-27T05:40:11.570598Z"
    }
   },
   "outputs": [],
   "source": [
    "naive_bayes = MultinomialNB(alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:40:13.539808Z",
     "start_time": "2020-02-27T05:40:11.578106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the empirical log probability of features given a class, $P(x_i|y)$ by `feature_log_prob_` attribute. We postulate that, for a term $v_i$, if $P(v_i|y_i=0)$ is very different from $P(v_i|y_i=1)$, then $v_i$ is likely to be a discriminating term across the classes. Hence, we compute\n",
    "\n",
    "$|P(v_i|y_i=0) - P(v_i|y_i=1)|$ for all $v_i\\in V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:40:13.550014Z",
     "start_time": "2020-02-27T05:40:13.543504Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_prob_cls_0, feature_prob_cls_1 = np.exp(naive_bayes.feature_log_prob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:40:13.596257Z",
     "start_time": "2020-02-27T05:40:13.552658Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_prob = pd.DataFrame(data={'feature': vectorizer.get_feature_names(),\n",
    "                                  'prob_cls_0': feature_prob_cls_0,\n",
    "                                  'prob_cls_1': feature_prob_cls_1})\n",
    "feature_prob['abs_prob_diff'] = np.abs(feature_prob['prob_cls_0'] - feature_prob['prob_cls_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:40:13.622127Z",
     "start_time": "2020-02-27T05:40:13.599374Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>prob_cls_0</th>\n",
       "      <th>prob_cls_1</th>\n",
       "      <th>abs_prob_diff</th>\n",
       "      <th>Belong_To_Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>like</td>\n",
       "      <td>0.017297</td>\n",
       "      <td>0.013247</td>\n",
       "      <td>0.004050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>quot</td>\n",
       "      <td>0.018090</td>\n",
       "      <td>0.021805</td>\n",
       "      <td>0.003715</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>great</td>\n",
       "      <td>0.004938</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>love</td>\n",
       "      <td>0.005198</td>\n",
       "      <td>0.008024</td>\n",
       "      <td>0.002826</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>good</td>\n",
       "      <td>0.010347</td>\n",
       "      <td>0.007762</td>\n",
       "      <td>0.002586</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>one</td>\n",
       "      <td>0.011837</td>\n",
       "      <td>0.014188</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>bad</td>\n",
       "      <td>0.003582</td>\n",
       "      <td>0.001522</td>\n",
       "      <td>0.002061</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>best</td>\n",
       "      <td>0.004976</td>\n",
       "      <td>0.006791</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>better</td>\n",
       "      <td>0.004705</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>music</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>0.010447</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    feature  prob_cls_0  prob_cls_1  abs_prob_diff  Belong_To_Class\n",
       "675    like    0.017297    0.013247       0.004050                0\n",
       "921    quot    0.018090    0.021805       0.003715                1\n",
       "502   great    0.004938    0.008425       0.003487                1\n",
       "695    love    0.005198    0.008024       0.002826                1\n",
       "492    good    0.010347    0.007762       0.002586                0\n",
       "818     one    0.011837    0.014188       0.002351                1\n",
       "80      bad    0.003582    0.001522       0.002061                0\n",
       "104    best    0.004976    0.006791       0.001815                1\n",
       "105  better    0.004705    0.002900       0.001805                0\n",
       "777   music    0.008701    0.010447       0.001745                1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_prob['Belong_To_Class'] = (feature_prob.prob_cls_0 > feature_prob.prob_cls_1).map({True: 0, False:1})\n",
    "feature_prob.sort_values('abs_prob_diff', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the metric we defined above, we identified the top 10 discriminating term and showed there association with a certain class. It is apparent that\n",
    "- The terms are distinct from what we identified in Q2 with lasso logistic regression\n",
    "- Although the 'discriminating' terms are pretty convince, as they are mostly very emotional words\n",
    "- The classification is not very accurate. We have used a very simple criteria: the class with higher probability for the term is assigned. The drawback is that sometimes bi-grams like \"not good\", \"don't love\" appear in the complimentary class (label 1), and we brutally picked it up with \"good\" and \"love\"\n",
    "\n",
    "This issue is associated with the fundamental independence assumption of Naive Bayes Classifier. Simply treating words as independent objects will ignore any negation or context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 Estimate Multinomial Inverse Regression model\n",
    "This part is implemented in R with `textir` package. We fitted a logistic regression as the forward regression.\n",
    "\n",
    "The terms with heaviest loadings (sorted by absolute value of coefs) are:\n",
    "- garbag\n",
    "- wack\n",
    "- horribl\n",
    "- worst\n",
    "- aw\n",
    "- terribl\n",
    "- wors\n",
    "- mediocr\n",
    "- suck\n",
    "- timeless\n",
    "\n",
    "They are all *strong* emotional wordings. The first nine (disapproving) are loaded with negative coefficients, while the last one 'timeless' is loaded with positive coefficient.\n",
    "\n",
    "The terms with lightest loadings are:\n",
    "- week\n",
    "- went\n",
    "... I will ignore the rest of them because they are all zeroed out.\n",
    "\n",
    "The in-sample accuracy score is **0.8307**, quite close to the in-sample goodness-of-fit of LASSO regression **0.8597**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 Out-of-sample CV comparison of LASSO, NB and MNIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:47:13.251188Z",
     "start_time": "2020-02-27T05:47:12.312748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO\n",
      "In-sample 0.860648418449703\n",
      "Out-of-sample 0.8573636223149436\n"
     ]
    }
   ],
   "source": [
    "print('LASSO')\n",
    "print('In-sample', accuracy_score(y_train, lasso_logistic.predict(X_train)))\n",
    "print('Out-of-sample', accuracy_score(y_test, lasso_logistic.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T05:40:14.651681Z",
     "start_time": "2020-02-27T05:40:14.108979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB\n",
      "In-sample 0.7949994848370368\n",
      "Out-of-sample 0.7853500231803431\n"
     ]
    }
   ],
   "source": [
    "print('NB')\n",
    "print('In-sample', accuracy_score(y_train, naive_bayes.predict(X_train)))\n",
    "print('Out-of-sample', accuracy_score(y_test, naive_bayes.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *in-sample* prediction accuracy (ranked) of\n",
    "- LASSO is **0.860648418449703**\n",
    "- MNIR is **0.830700278188**\n",
    "- NB is **0.7949994848370368**\n",
    "\n",
    "The *out-of-sample* prediction accuracy (ranked) of\n",
    "- LASSO is **0.8573636223149436**\n",
    "- NB is **0.7853500231803431**\n",
    "- MNIR is **0.751090428272143**\n",
    "\n",
    "Findings:\n",
    "- Unanimously, the out-of-sample accuracy is lower than in-sample.\n",
    "- The LASSO model outperforms the other two both in- and out-of sample. Though MNIR does better in-sample, Naive Bayes Classifier does better out-of-sample."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
